import * as faceapi from 'face-api.js';

let modelsLoaded = false;

/**
 * Carga los modelos de reconocimiento facial en el cliente (navegador).
 * Utiliza Face-API.js sobre TensorFlow.js.
 * Implementa una estrategia de redundancia (fallback) para intentar cargar los modelos
 * desde m√∫ltiples fuentes (Nube o Local) si alguna falla.
 * 
 * @returns {Promise<Object>} - Resultado de la carga {success: boolean, error?: string}
 */
export const loadModelsLocal = async () => {
    if (modelsLoaded) return { success: true };

    try {
        console.log("üöÄ Forzando motor GPU (WebGL) para Face-API...");
        // Configura el backend de TensorFlow para usar aceleraci√≥n por hardware (WebGL)
        await faceapi.tf.setBackend('webgl');
        await faceapi.tf.ready();
        console.log("‚úÖ Motor GPU activo:", faceapi.tf.getBackend());
    } catch (e) {
        console.warn("‚ö†Ô∏è No se pudo activar WebGL, usando motor por defecto (CPU o WASM):", e.message);
    }

    /**
     * Funci√≥n auxiliar para verificar si existe el archivo de modelos via HTTP antes de intentar cargarlo.
     * Esto evita errores internos de Face-API dif√≠ciles de capturar.
     */
    const verifyAndLoad = async (baseUrl) => {
        const manifestUrl = `${baseUrl}/tiny_face_detector_model-weights_manifest.json`;
        console.log(`üîç Verificando acceso a modelos en: ${manifestUrl}`);

        try {
            // 1. Pre-check: Fetch simple para ver si el archivo existe y es accesible
            const controller = new AbortController();
            const timeoutId = setTimeout(() => controller.abort(), 5000); // 5s timeout de red

            const response = await fetch(manifestUrl, {
                method: 'GET',
                signal: controller.signal
            });
            clearTimeout(timeoutId);

            if (!response.ok) {
                throw new Error(`Status HTTP ${response.status} (${response.statusText})`);
            }

            // Verificamos que no nos devuelva una p√°gina HTML (com√∫n en errores 404 de SPA)
            const contentType = response.headers.get('content-type');
            if (contentType && contentType.includes('text/html')) {
                throw new Error(`Detectado HTML en lugar de JSON. Posible 404 o redirecci√≥n.`);
            }

            // Validar que sea JSON v√°lido
            try {
                const clone = response.clone();
                await clone.json();
            } catch (jsonErr) {
                throw new Error(`El archivo manifiesto no es un JSON v√°lido.`);
            }

            console.log(`‚úÖ Pre-check OK para ${baseUrl}. Iniciando carga de modelos...`);

            // 2. Carga real via Face-API
            // Cargamos: Detector ligero (Tiny), Puntos faciales (Landmark), Reconocimiento (Recognition)
            await Promise.all([
                faceapi.nets.tinyFaceDetector.loadFromUri(baseUrl),
                faceapi.nets.faceLandmark68Net.loadFromUri(baseUrl),
                faceapi.nets.faceRecognitionNet.loadFromUri(baseUrl)
            ]);

            console.log(`üèÜ CARGA EXITOSA desde: ${baseUrl}`);
            return baseUrl;

        } catch (error) {
            console.warn(`‚ö†Ô∏è Fall√≥ intento de carga desde ${baseUrl}: ${error.message}`);
            throw error; // Relanzar para que Promise.any pruebe el siguiente
        }
    };

    const origin = window.location.origin;
    const base = origin.endsWith('/') ? origin.slice(0, -1) : origin;

    try {
        // Intentamos cargar desde m√∫ltiples fuentes en paralelo hasta que una funcione (Promise.any)
        await Promise.any([
            verifyAndLoad('https://recofacial-7cea1.web.app/models'), // 1. Nube (Firebase Hosting) - Prioridad por velocidad/cache
            verifyAndLoad(`${base}/models`),                          // 2. Local App Absoluto
            verifyAndLoad('/models'),                                 // 3. Local Web Relativo
            verifyAndLoad('models')                                   // 4. Fallback final
        ]);

        modelsLoaded = true;
        return { success: true };
    } catch (aggregateError) {
        console.error("‚ùå TODOS LOS INTENTOS DE CARGA FALLARON", aggregateError);

        // Generar reporte de error detallado
        let errorMsg = "No se pudieron cargar los modelos de IA.\n";
        if (aggregateError.errors) {
            aggregateError.errors.forEach((e, i) => {
                errorMsg += `\nFuente ${i + 1}: ${e.message}`;
            });
        } else {
            errorMsg += aggregateError.message;
        }

        return {
            success: false,
            error: errorMsg
        };
    }
};

/**
 * Procesa la imagen del video directamente en el navegador para obtener datos faciales.
 * 
 * @param {HTMLVideoElement} videoElement - Elemento de video HTML5.
 * @returns {Promise<Object|null>} - Objeto con { descriptor, detection } o null si no hay rostro.
 */
export const getFaceDataLocal = async (videoElement) => {
    if (!videoElement) return null;

    try {
        // Opciones optimizadas para velocidad: inputSize peque√±o (320px)
        const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 320, scoreThreshold: 0.5 });

        // Detecci√≥n + Puntos Faciales + Descriptor (Vector de 128 floats)
        const result = await faceapi.detectSingleFace(videoElement, options)
            .withFaceLandmarks()
            .withFaceDescriptor();

        if (result) {
            return {
                descriptor: result.descriptor,
                detection: result.detection // Contiene el cuadro (box) con width, height, etc. para validar calidad
            };
        }
        return null; // No se detect√≥ rostro
    } catch (err) {
        console.error("Error en detecci√≥n local:", err);
        return null;
    }
};

/**
 * Wrapper simplificado para obtener solo el descriptor.
 */
export const getFaceDescriptorLocal = async (videoElement) => {
    const data = await getFaceDataLocal(videoElement);
    return data ? data.descriptor : null;
};
